{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ISCd4PeQwQ6",
        "outputId": "8876ef30-bd1c-4616-fac4-4315b3cce754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=bcc4435805a0f0c4ddfe109124311277a05e9e152295bb082449ca1d4457afa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 81 kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.6\n"
          ]
        }
      ],
      "source": [
        "# install\n",
        "\n",
        "!pip install sklearn\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZlzqnmEJM2a",
        "outputId": "228c52c8-577c-4bbb-f6fa-a7e91ef7d831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "#from tensorflow.keras.utils.vis_utils import plot_model\n",
        "from catboost import CatBoostClassifier\n",
        "#from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_auc_score, SCORERS, RocCurveDisplay, roc_curve\n",
        "from sklearn.model_selection import train_test_split,KFold, RandomizedSearchCV\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dMROE0bUOOZh",
        "outputId": "525fda01-ddf5-4483-bc07-11bff74b4137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9MhqEC9GOBO"
      },
      "outputs": [],
      "source": [
        "#     ***** FEATURES ORDER: *****\n",
        "\n",
        "#     \"num_page\": num_page,\n",
        "#     \"num_category\": len(cat_dict), \n",
        "#     \"max_category\": cat_dict[max_cat], \n",
        "#     \"num_product\": len(product_dict),\n",
        "#     \"max_product\": product_dict[max_product],\n",
        "#     \"cur_type\": row['event_type'], \n",
        "#     \"num_cart\": type_dict['cart'],\n",
        "#     \"num_remove_cart\": type_dict['remove_from_cart'],\n",
        "#     \"num_view\": type_dict['view'],\n",
        "#     \"price\": row['price'],\n",
        "#     \"max_price\": max_price,\n",
        "#     \"min_price\": min_price,\n",
        "#     \"mean_price\": mean_price \n",
        "# \n",
        "#     ***** EVENT TYPES: *****\n",
        "# cart = 0, removecart = 1, view = 2, purchase = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb3KeS8vstWU"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path):\n",
        "    with open(path, 'rb') as handle:\n",
        "        X = pickle.load(handle)\n",
        "    return X\n",
        "\n",
        "\n",
        "# make balanced dataset by dropping some zero classes (not randomly)\n",
        "def balance_dataset(X, Y):\n",
        "    count_zeros = len(Y) - int(np.sum(Y))\n",
        "    c = 0\n",
        "    X_input = []\n",
        "    Y_input = []\n",
        "    for (x,y) in zip(X,Y):\n",
        "        if not int(y) or c < count_zeros:\n",
        "            Y_input.append(y)\n",
        "            X_input.append(x)\n",
        "            if int(y):\n",
        "                c += 1\n",
        "    X = np.array(X_input)\n",
        "    Y = np.array(Y_input)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "# make balanced dataset using oversampling\n",
        "def oversampling_balance(X, Y):\n",
        "    org_shape = X.shape\n",
        "    X = X.reshape((X.shape[0], X.shape[1]*X.shape[2]))\n",
        "    ros = RandomOverSampler(sampling_strategy='minority')\n",
        "    X_resampled, y_resampled = ros.fit_resample(X, Y)\n",
        "    return X_resampled.reshape(org_shape), y_resampled\n",
        "\n",
        "\n",
        "# extract a balanced test set from dataset\n",
        "def balanced_testset(ds, split_ratio):\n",
        "    split = int((len(ds)*split_ratio)/2)\n",
        "    c0, c1 = 0, 0\n",
        "    l0, l1 = [], []\n",
        "    for d in ds:\n",
        "        if d[0,-1] == 1 and c1 < split:\n",
        "            l1.append(True)\n",
        "            c1 += 1\n",
        "        else:\n",
        "            l1.append(False)\n",
        "        if d[0,-1] == 0 and c0 < split:\n",
        "            l0.append(True)\n",
        "            c0 += 1\n",
        "        else:\n",
        "            l0.append(False)\n",
        "    testds = ds[np.any([l0,l1], axis=0)]\n",
        "    trainds = ds[np.any([l0,l1], axis=0) == False]\n",
        "    return testds, trainds\n",
        "\n",
        "# extract X and Y label from all sessions\n",
        "def extract_XY(X):\n",
        "    Y = X[:,0,-1]\n",
        "    Y = np.array([y for y in Y])\n",
        "    X = X[:,:,:-1]\n",
        "    return X, Y\n",
        "\n",
        "# split dataset to test, validation, train\n",
        "def split_dataset(X, Y, test_size, val_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = val_size)\n",
        "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
        "\n",
        "\n",
        "# for Catboost model only the last page view and its features is needed\n",
        "def make_dataset2(ds):\n",
        "    dataset2 = []\n",
        "    for session in ds:\n",
        "        end = False\n",
        "        for idx, page in enumerate(session):\n",
        "            if page[0] == 0.:\n",
        "                dataset2.append(session[idx-1])\n",
        "                end = True\n",
        "                break \n",
        "        if end == False:\n",
        "            dataset2.append(session[-1])    \n",
        "    dataset2 = np.array(dataset2)\n",
        "    return dataset2\n",
        "\n",
        "# extract X and Y from catboost dataset\n",
        "def extract_CXY(ds):\n",
        "    CY = ds[:,-1]\n",
        "    CY = np.array([y for y in CY])\n",
        "    CX = ds[:,:-1]\n",
        "    return CX, CY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention layer\n",
        "\n",
        "class attention(layers.Layer):\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "        super(attention,self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
        "        super(attention,self).build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        return K.sum(output, axis=1)"
      ],
      "metadata": {
        "id": "-6nbcUEdm9Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm5RnSHDs5TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d80754-de56-4604-88cd-5cdd37b1804e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of dataset for GRU is:  <class 'numpy.ndarray'>\n",
            "Shape of dataset for GRU is:  (408124, 100, 14)\n",
            "******************************\n",
            "size of class 1:  331800\n",
            "size of class 0:  76324\n",
            "Total:  408124\n",
            "******************************\n",
            "shape of X is: (408124, 100, 13) and shape of Y is (408124,)\n"
          ]
        }
      ],
      "source": [
        "# load train dataset\n",
        "# dataset = load_dataset('/content/drive/MyDrive/purchase_prediction_features.pickle')\n",
        "dataset = load_dataset('/content/drive/MyDrive/dataset_karamuzi/purchase_prediction_trainset.pickle')\n",
        "print(\"Type of dataset for GRU is: \", type(dataset))\n",
        "print(\"Shape of dataset for GRU is: \", dataset.shape)\n",
        "print(\"*\"*30)\n",
        "\n",
        "# train dataset labels count\n",
        "c = int(np.sum(dataset[:,0,-1]))\n",
        "print(\"size of class 1: \", c)\n",
        "print(\"size of class 0: \", len(dataset) - c)\n",
        "print(\"Total: \", len(dataset))\n",
        "print(\"*\"*30)\n",
        "\n",
        "# extract X and Y for all sessions in train dataset\n",
        "X, Y = extract_XY(dataset)\n",
        "print(f\"shape of X is: {X.shape} and shape of Y is {Y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ********** exctract a testset from dataset using balanced_testset() function **********\n",
        "# run this block only one time\n",
        "# first uncomment load_dataset('/content/drive/MyDrive/purchase_prediction_features.pickle') \n",
        "# and comment load_dataset('/content/drive/MyDrive/dataset_karamuzi/purchase_prediction_trainset.pickle') from above block\n",
        "# uncomment codes of this block and run\n",
        "# save the results to purchase_prediction_testset.pickle and purchase_prediction_trainset.pickle\n",
        "# then comment this block and uncommented load_dataset of above block\n",
        "# now you can run above block and get train set and test set from drive\n",
        "\n",
        "# test_set, train_set = balanced_testset(dataset, 0.02)\n",
        "\n",
        "# path = '/content/drive/MyDrive/purchase_prediction_testset.pickle'\n",
        "# with open(path, 'wb') as f:\n",
        "#     pickle.dump(test_set, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# path = '/content/drive/MyDrive/purchase_prediction_trainset.pickle'\n",
        "# with open(path, 'wb') as f:\n",
        "#     pickle.dump(train_set, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "VRvvAH8KMj3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test dataset\n",
        "testdata = load_dataset('/content/drive/MyDrive/dataset_karamuzi/purchase_prediction_testset.pickle') \n",
        "print(\"Type of test dataset for GRU is: \", type(testdata))\n",
        "print(\"Shape of test dataset for GRU is: \", testdata.shape)\n",
        "print(\"*\"*30)\n",
        "\n",
        "# test dataset labels count\n",
        "c = int(np.sum(testdata[:,0,-1]))\n",
        "print(\"size of class 1: \", c)\n",
        "print(\"size of class 0: \", len(testdata) - c)\n",
        "print(\"Total: \", len(testdata))\n",
        "print(\"*\"*30)\n",
        "\n",
        "\n",
        "# extract X and Y for all sessions in train dataset\n",
        "Xtest, Ytest = extract_XY(testdata)\n",
        "print(f\"shape of X test is: {Xtest.shape} and shape of Y test is {Ytest.shape}\")"
      ],
      "metadata": {
        "id": "hsvzR_C85O3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df582d2-df4b-435f-ab88-e0a3543b0928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of test dataset for GRU is:  <class 'numpy.ndarray'>\n",
            "Shape of test dataset for GRU is:  (8328, 100, 14)\n",
            "******************************\n",
            "size of class 1:  4164\n",
            "size of class 0:  4164\n",
            "Total:  8328\n",
            "******************************\n",
            "shape of X test is: (8328, 100, 13) and shape of Y test is (8328,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfWMxj3HzxAB"
      },
      "outputs": [],
      "source": [
        "# add layers of GRU model in this function\n",
        "# layers:\n",
        "#   input layer\n",
        "#   mask layer\n",
        "#   GRU layer\n",
        "#   dropout layer\n",
        "#   pooling layer\n",
        "#   Flatten layer\n",
        "#   dense layer (feed forward)\n",
        "#   dense layer (sigmoid)\n",
        "\n",
        "def GRU_model(input_shape, units, lr, dr, pooling):\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(layers.Input(shape = (input_shape[0], input_shape[1])))\n",
        "\n",
        "    model.add(layers.Masking(mask_value=0.))\n",
        "\n",
        "    for u in units:\n",
        "        model.add(layers.GRU(u, return_sequences=True))\n",
        "\n",
        "    # model.add(attention())\n",
        "\n",
        "    if dr != 0:\n",
        "        model.add(layers.Dropout(dr))\n",
        "\n",
        "    if pooling == 'max':\n",
        "        model.add(layers.MaxPooling1D(pool_size=3))\n",
        "    elif pooling == 'avg':\n",
        "        model.add(layers.AveragePooling1D(pool_size=3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', 'AUC'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb2e2RCUVden"
      },
      "outputs": [],
      "source": [
        "def run_simple_GRU(units, lr, dr, pooling):\n",
        "\n",
        "    # input shape to pass to GRU model\n",
        "    input_shape = [X.shape[1], X.shape[2]]\n",
        "\n",
        "    # split train dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.05, random_state=1)\n",
        "\n",
        "    # build GRU model\n",
        "    model = GRU_model(input_shape, units, lr, dr, pooling)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # add early stopping , set patience to 5\n",
        "    cb = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, mode='max')\n",
        "\n",
        "    # add class weight\n",
        "    c1 = int(np.sum(y_train))\n",
        "    c0 = len(y_train) - c1\n",
        "    cw = {0 : c1/len(y_train), 1 : c0/len(y_train)}\n",
        "\n",
        "    # fit model with early stopping and class weight , I set batch size to 32 and epochs to 50\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, \n",
        "                        epochs=50, class_weight=cw, verbose=1, callbacks=[cb])\n",
        "\n",
        "    # predict on test\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # calculate auc\n",
        "    auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "    print(\"AUC = \", auc)\n",
        "\n",
        "    # RocCurveDisplay.from_predictions(y_test, preds)\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(history.history['accuracy'])\n",
        "    # plt.plot(history.history['val_accuracy'])\n",
        "    # plt.title('model accuracy')\n",
        "    # plt.ylabel('accuracy')\n",
        "    # plt.xlabel('epoch')\n",
        "    # plt.legend(['train', 'test'], loc='upper left')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(history.history['loss'])\n",
        "    # plt.plot(history.history['val_loss'])\n",
        "    # plt.title('model loss')\n",
        "    # plt.ylabel('loss')\n",
        "    # plt.xlabel('epoch')\n",
        "    # plt.legend(['train', 'test'], loc='upper left')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameters\n",
        "# learning_rate : [0.005, 0.001, 0.0001, 0.0005]\n",
        "# dropout : [0, 0.2, 0.25, 0.5]\n",
        "# GRU layers : [1, 2, 3] --> hidden nodes of each layer: 128, 64, 32\n",
        "# pooling : [max , mean , none]\n",
        "\n",
        "units = [128, 64]\n",
        "lr = 0.001\n",
        "dr = 0.2\n",
        "pooling = 'max'\n",
        "\n",
        "run_simple_GRU(units, lr, dr, pooling)"
      ],
      "metadata": {
        "id": "VHl8l7aIYBQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kfold_GRU(units, lr, dr, pooling):\n",
        "\n",
        "    # same as run_simple_GRU but run 'num_folds' times\n",
        "        \n",
        "    input_shape = [X.shape[1], X.shape[2]]\n",
        "\n",
        "    # save results of each fold to outs\n",
        "    outs = []\n",
        "\n",
        "    num_folds = 10\n",
        "    fold_no = 0\n",
        "    mean_auc = 0\n",
        "\n",
        "    # k fold cross validation\n",
        "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "    for train, test in kfold.split(X, Y):\n",
        "\n",
        "        fold_no += 1\n",
        "\n",
        "        print(\"#\"*50)\n",
        "\n",
        "        model = GRU_model(input_shape, units, lr, dr, pooling)\n",
        "\n",
        "        print(model.summary())\n",
        "\n",
        "        cb = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=5, mode='max')\n",
        "\n",
        "        c1 = int(np.sum(Y[train]))\n",
        "        c0 = len(Y[train]) - c1\n",
        "        cw = {0 : c1/len(Y[train]), 1 : c0/len(Y[train])}\n",
        "\n",
        "        history = model.fit(X[train], Y[train], validation_data=(X[test], Y[test]), batch_size=32, \n",
        "                            epochs=50, class_weight=cw, verbose=1, callbacks=[cb])\n",
        "\n",
        "        preds = model.predict(X[test])\n",
        "\n",
        "        auc = roc_auc_score(Y[test], preds)\n",
        "\n",
        "        outs.append(auc)\n",
        "\n",
        "        mean_auc += auc\n",
        "\n",
        "    mean_auc = mean_auc / 10\n",
        "\n",
        "    for i, x in enumerate(outs):\n",
        "        print(f\"GRU fold {i}\", \"  AUC = \", x)\n",
        "\n",
        "    print(\"GRU Mean AUC: \", mean_auc)"
      ],
      "metadata": {
        "id": "AlzREwFL6cjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build catboost train dataset \n",
        "Cds = make_dataset2(dataset) \n",
        "print(\"Shape of dataset for Catboost is: \", Cds.shape) \n",
        "\n",
        "# extract X and Y from catboost train dataset\n",
        "CX, CY = extract_CXY(Cds)\n",
        "print(f\"shape of CX is: {CX.shape} and shape of CY is {CY.shape}\")"
      ],
      "metadata": {
        "id": "F3vkyh4R5qo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607e8119-86b9-45bb-e0d4-134226d674e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dataset for Catboost is:  (408124, 14)\n",
            "shape of CX is: (408124, 13) and shape of CY is (408124,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build catboost test dataset \n",
        "Cdstest = make_dataset2(testdata) \n",
        "print(\"Shape of test dataset for Catboost is: \", Cdstest.shape) \n",
        "\n",
        "# extract X and Y from catboost test dataset\n",
        "CXt, CYt = extract_CXY(Cdstest)\n",
        "print(f\"shape of CXt is: {CXt.shape} and shape of CYt is {CYt.shape}\")"
      ],
      "metadata": {
        "id": "7T6mzNXy5s6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5286e97d-0dfd-413c-dccb-f0838a9c25eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test dataset for Catboost is:  (8328, 14)\n",
            "shape of CXt is: (8328, 13) and shape of CYt is (8328,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_catboost():\n",
        "    \n",
        "    # add class weight \n",
        "    classes = np.unique(CY)\n",
        "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=CY)\n",
        "    class_weights = dict(zip(classes, weights))\n",
        "\n",
        "    cbc_model = CatBoostClassifier(class_weights=class_weights)\n",
        "\n",
        "    # hyper parameters for random search\n",
        "    param_dist = { \"learning_rate\": [0.1, 0.01, 0.05, 0.005, 0.001], \n",
        "                \"max_depth\": [6, 8, 10], 'subsample': [0.4, 0.6, 0.8] }\n",
        "\n",
        "    #Instantiate RandomSearchCV object\n",
        "    # with K-fold cross validation\n",
        "    rscv = RandomizedSearchCV(estimator=cbc_model, param_distributions=param_dist, scoring='roc_auc', cv=10)\n",
        "\n",
        "    # fit model and add early stopping (patience = 5)\n",
        "    rscv.fit(CX, CY, early_stopping_rounds=5, plot=True)\n",
        "\n",
        "    # Print the tuned parameters and score\n",
        "    print(\"Catboost best params\", rscv.best_params_)\n",
        "    print(\"Catboost best scores\", rscv.best_score_)"
      ],
      "metadata": {
        "id": "i4bXqJPQ3j7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_catboost():\n",
        "\n",
        "    # run catboost with best parameters and evaluate on test dataset\n",
        "\n",
        "    CX_train, CX_test, Cy_train, Cy_test = train_test_split(CX, CY, test_size = 0.05, random_state=1)\n",
        "\n",
        "    model = CatBoostClassifier(learning_rate=0.05,\n",
        "                                subsample=0.8,\n",
        "                                max_depth=6,\n",
        "                                eval_metric='AUC',\n",
        "                                task_type=\"GPU\",\n",
        "                                bootstrap_type='Poisson',\n",
        "                                devices='0:1')\n",
        "\n",
        "    model.fit(CX_train, Cy_train, eval_set=(CX_test, Cy_test))\n",
        "\n",
        "    preds = model.predict_proba(CXt)\n",
        "\n",
        "    auc = roc_auc_score(CYt, preds[:,1])\n",
        "\n",
        "    print(\"AUC = \", auc)"
      ],
      "metadata": {
        "id": "9Xnqzsq_7C8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlHVp6LQdAyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}